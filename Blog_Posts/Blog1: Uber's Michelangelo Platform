Decoding Michelangelo: Uber’s Machine Learning Platform
 By Ishaan Gupta 

When you open your Uber app and see “Driver arriving in 3 minutes,” you’re not just looking at GPS coordinates. That number is the outcome of hundreds of machine learning models running in real time across a massive global infrastructure. The cost of a wrong prediction here goes far beyond a few extra minutes. It can mean losing a customer, frustrating a driver, or millions in lost revenue when mistakes add up at scale, leading to serious business drawbacks.
That’s why Uber relies on its secret weapon: Michelangelo( ML-as-a-service platform), a production-grade ML system that powers the company’s operations. This system is responsible for delivering accurate and fast predictions by ensuring that the right data flows in, the right models get trained, and the right predictions reach your phone within milliseconds.
Over the years, Uber has fundamentally transformed how society moves and travels, and that has been possible only because of its relentless focus on real-time logistics and continuous system improvements. 
In this article, we’ll explore the development of Uber’s Michelangelo architecture and how it leverages MLOps to ensure every ride is on time and every meal arrives just right.
Around the time of Uber’s explosive growth pre-2015, the company’s main focus was rapid product expansion : launching in new cities, and improving customer experience. While this growth looked smooth from the outside, it exposed deep limitations in Uber’s technical architecture, especially its machine learning infrastructure.
At the time, ML work was fragmented across teams:
Local experiments: Data scientists often built models in R or scikit-learn, running them locally on their laptops.
Manual deployments: Engineers had to create custom, one-off systems every time a model was deployed.
No standardization: There were no unified pipelines or standard methods for tracking experiments or storing training results.
Poor prioritization: All models were treated equally, so a $10 revenue model got the same attention as one driving $10M.
This lack of maturity wasn’t unusual for the time. MLOps was still in its infancy, tools for managed operations didn’t exist, and the pace of product growth left little time to design thoughtful, scalable systems.
But it created risk: Uber was falling into common ML anti-patterns like 
ad-hoc workflows
duplicated efforts
Models and pipelines that couldn’t be reproduced reliably and scaled easily. 

Later, in the years after 2015, Uber’s operations expanded at a breakneck pace. Millions of trips were being booked daily across dozens of countries, and with that came a flood of data.
To keep customers happy and drivers engaged, Uber needed more than ad-hoc models and basic systems. It required an architecture that could standardize workflows, unify tools across teams, and deliver accurate predictions for ETAs, fraud detection, and countless other real-time decisions at scale.
Building on recent advances in technology, Uber’s engineers created Michelangelo, an end-to-end ML platform that streamlined the workflow from data ingestion to prediction delivery, structured around a standardized six-step process.
Manage data
Train models
Evaluate models
Deploy models
Make predictions
Monitor predictions
Let’s now dive into how Michelangelo’s architecture powers every step of this workflow.
Manage Data 
As we already know, Uber runs a wide range of machine learning use cases, from demand forecasting and surge pricing to fraud detection, ETA prediction, and personalized recommendations. 
Each prediction serves a different purpose, and due to this, each use case relies on a unique set of features, many of which must be calculated in specific ways.
To address this challenge, Uber built Michelangelo’s data management components around a dual-pipeline architecture, allowing it to generate the features needed to keep predictions accurate and reliable.
Offline Pipelines (HDFS + Spark)
First of all, transactional and log data flows into Uber’s Data Lake, which is built on HDFS (Hadoop Distributed File System). HDFS provides the distributed and fault-tolerant storage layer that allows Uber to store and manage massive volumes of raw data.
From there, data scientists write Hive SQL queries, which are executed using Spark’s distributed, parallel computation engine. Spark basically distributes work across many servers, efficiently transforming Uber’s massive datasets into features such as average driver acceptance rates or long-term demand patterns.

These features then power two critical workflows:
Batch training jobs    :  training models on labeled historical data, allowing them to capture complex patterns in demand and user behavior.
Batch prediction jobs :  generating forecasts in bulk, such as nightly churn predictions or next-day demand estimates for specific regions.
These pipelines are highly automated, with jobs running on a fixed schedule or triggered immediately when new data arrives.
Online Pipelines (Kafka + Samza + Cassandra)
Unlike offline pipelines, which are designed for training models on historical data or generating bulk forecasts, some predictions and feature calculations at Uber need to happen instantly. This is why the offline setup we discussed earlier isn’t enough.
To meet these requirements, Uber built a streaming architecture powered by Kafka, Samza, and Cassandra:
Kafka : collects real-time events from Uber’s services and streams them as a continuous, high-throughput flow streams(e.g., a restaurant starting an order, a driver going online).
Samza  : consumes these event streams and computes features in real time (e.g., average meal prep time over the last one hour).
Cassandra :  stores these computed features in a low-latency NoSQL database, making them instantly available to models during prediction.
Now, when a customer places an order, Uber’s ML models already have the most recently updated features available in milliseconds, enabling real-time ETAs and price predictions. 
And to further optimize how these online features are served, Uber follows two key strategies, which we will now explore.
Uber’s Two Strategies for Serving Online Features
Uber uses a mix of batch precompute and near-real-time compute, depending on how frequently a feature needs to be updated:
1. Batch Precompute
Features are bulk-computed from HDFS and periodically loaded into Cassandra (e.g., daily or weekly).
Works well for slow-changing features that don’t need to be refreshed constantly.
Example: average prep time over the last 7 days.
Advantage: delivers real-time predictions without overloading the system with unnecessary complexity or resource use.

2. Near-Real-Time Compute
Features are updated continuously as events happen. (e.g., every hour daily)
Metrics from Uber’s services are published to Kafka, then processed in real time by Samza and then accessed through Cassandra.
The same data is also logged back into HDFS for use in retraining.
This approach is essential for fast-changing features that must always reflect the most recent state.

​​The Feature Store — Uber’s In-House Innovation
The team at Uber also recognized that many modeling problems rely on identical or very similar features. For instance, average meal preparation time at a restaurant can be valuable for separate model projects such as predicting food delivery ETAs or ranking restaurants by efficiency. 
So, instead of every team rebuilding the same logic again and again, there was huge value in creating a way to share features across projects and even across different organizations within Uber.
That’s why they developed the Feature Store which is a central hub where teams can create, manage, and reuse canonical features.
Here’s how it works:
Publishing features is easy.
Teams can add the features they build into the Feature Store with just a bit of extra metadata, such as owner, description, or SLA.
Consuming features is even easier.
Once a feature is in the Feature Store, any team can use it simply by referencing its canonical name in their model configuration (the file outlining details of a training job), and the rest of the back end is automatically handled by the Feature Store’s code.





2. Model Training in Michelangelo
Once features are prepared, the next step is training models that can capture patterns in Uber’s vast and dynamic data. Michelangelo supports offline, large-scale distributed training for a wide range of model types  : from decision trees to unsupervised models like k-means, time series forecasting, and even deep neural networks.
Training jobs run on clusters of machines (powered by YARN or Mesos) to handle Uber’s massive scale. Instead of running everything on a single server, the workload is distributed across many machines, allowing models to be trained on billions of samples while still supporting smaller, faster experiments when needed.
At the core of every training job is a model configuration file. This file defines the model type, hyperparameters, DSL expressions (model specific feature transformations), and the compute resources required.
Submitting this configuration to Michelangelo automatically launches a distributed training job on Uber’s clusters.
Special Features That Make Training Smarter
Hyperparameter Search
Every ML model has tuning parameters (like learning rate, depth of a tree, or number of clusters). Instead of data scientists trying combinations manually, Michelangelo runs many jobs in parallel, automatically testing parameter sets and selecting the best-performing model to parameter combination.

Partitioned Models
Uber’s business varies greatly by geography and context. For example, predicting ETAs in New York requires very different data patterns than in San Francisco or Mumbai. 
A single global model might miss these local nuances.
To solve this, Michelangelo supports partitioned models. Training data is automatically split based on user-defined configurations (e.g., city) and this approach lets Uber capture local behaviours where possible. 
If a partition doesn’t have enough data (like a small town), the system falls back to a parent model (e.g., state- or country-level).


Finally, teams can initiate and monitor training through the Web UI for visual oversight or through APIs for automated retraining. This flexibility supports everything from quick experiments to large-scale production workflows.


3. Model Evaluation and Reporting
Training a model is rarely a one-shot process. At Uber, it’s often a methodical exploration, experimenting with different features, algorithms, and hyperparameters to find the best-performing setup.
This hit-and-trial approach means engineers may train hundreds of models that never reach production, but even those “failures” are valuable because they guide teams toward better configurations for future models. 
Managing and comparing so many models is a challenge in itself. Michelangelo solves this by storing every trained model as a versioned object in its Model Repository (backed by Cassandra). 
The repository makes it easy to inspect and compare models side by side, helping engineers learn from past experiments and zero in on the configurations that deliver the best performance. 
For every model stored, the Model Repository keeps a detailed record, organized into:
1. Training Metadata
Who trained the model
Start and end time of the training job
Full configuration (features, hyperparameters, compute setup)
2. Data References
Links to training and test datasets
Distribution of each feature
Relative importance of features, including:
Partial dependence plots : show how a feature affects predictions while holding others constant
Distribution histograms :  illustrate how feature values are spread across the dataset
3. Performance Metrics
Model-specific accuracy metrics and reports:
Regression models → fitted vs. residual plots, train/test error percentiles, absolute error percentiles
Decision trees → visualization tools to understand and debug model behavior
Standard evaluation charts: ROC curves, PR curves, confusion matrices


4. Model Artifacts
Full learned parameters (weights, splits, embeddings)
Summary statistics for visualization

Whenever needed, all this information is easily available from the repository through a web UI (for inspection and comparison) and an API (for programmatic access).
Overall, Michelangelo turns model evaluation into a transparent process. By storing detailed records, generating accuracy reports, and providing visualization tools, it helps Uber’s data scientists not only pick the best model but also understand why it performs the way it does.


Regression model reports show regression-related performance metrics.


 Features, their impact on the model, and their interactions can be explored though a feature report.
4. Model Deployment in Michelangelo
As is well known, Uber operates across many services such as rides, food delivery, and freight, and each use case requires a different deployment approach to balance speed, scale, and flexibility. 
Michelangelo supports three main deployment modes:
1. Offline Deployment
The model runs in a Spark job inside an offline container, making batch predictions on large datasets. This setup is ideal for long-term forecasts and planning, where immediate results aren’t needed but large-scale processing is.
Example: Each night at 2 AM, Uber predicts which drivers are most likely to churn next month using historical data.

2. Online Deployment
The model is deployed to an online prediction service cluster (hundreds of machines) and is able to handle real-time or near real-time predictions.
Example:
When you place an UberEATS order, the ETA is predicted instantly by sending features (restaurant ID, order size, etc.) to the model via a network call (RPC/HTTP/gRPC).
3. Library Deployment

The model is packaged as a library (Java JAR) that can be embedded directly into another service. It’s then invoked like calling a Java API function, not a network request to make predictions faster. 
Example: A fraud detection service embeds the model library inside its code to score transactions instantly, allowing Uber to prevent fraudulent activity in real time without relying on external calls.

All these deployments are initiated and scheduled through automation scripts by many teams via Michelangelo’s API, and in some cases through the web UI commands, especially for the UberEATS delivery time models.

 Models from the model repository are deployed to online and offline containers for serving.
5. Making Predictions
Once deployed, a model in its serving container makes predictions by turning raw feature data into model-ready inputs. 
Features come from pipelines (for batch use cases) or client services (for real-time use cases), then pass through DSL transformations for cleaning, formatting, and consistency into model-ready input formats. 
The system then assembles these into a feature vector, which the model scores either instantly for online predictions or in bulk for offline predictions written back to Hive.


6. Referencing Models
At Uber’s scale, deploying new models or running multiple versions at once must happen without downtime or failures. Michelangelo enables this through a few key mechanisms:
1. Model Identification
Each model has a unique ID (UUID) and an optional tag (alias).
Online models: The UUID or tag included with the feature vector automatically routes the prediction request to the correct model version.
Batch jobs: Multiple models can score the same dataset, with results labeled by UUID or tag for comparison.

2. Seamless Switching
When a new model uses the same features as an old one, it can be deployed to the same tag. The serving container then instantly begins using the new model with no client code changes, reducing friction and speeding adoption.
3. Gradual Rollouts
New models can also be deployed under a separate UUID, with traffic shifted gradually from the old model to the new one. This staged rollout lowers risk while maintaining continuity.
4. A/B Testing at Scale
Teams can deploy multiple models in parallel and route portions of live traffic to each, enabling safe, large-scale experimentation and data-driven improvements in model evaluation and testing.

Summary
This setup minimizes risk, reduces downtime, supports rapid iteration, and ensures that customer-facing predictions, from ETAs to fare estimates, remain accurate and scalable as models evolve.

Scaling in Michelangelo
Since machine learning models are stateless and share nothing, they are trivial to scale out. Uber applies this principle to both its online and offline pipelines. 
For online models, scaling is achieved by simply adding more hosts to the prediction service cluster and letting the load balancer distribute the requests. 
For offline predictions, additional Spark executors can be added, with Spark handling the parallelism automatically.

Monitoring Predictions
At Uber, millions of real-time decisions power services like driver matching and food delivery estimates, making prediction accuracy critical.
These services are constantly influenced by factors such as traffic, weather, driver availability, and customer demand. Since models are trained on historical data, continuous monitoring is essential to ensure predictions remain relevant as conditions change.
Equally important is the reliability of the data pipelines feeding these models. If pipelines fail or data quality drops, even the most accurate models will produce poor results.
To address these challenges, Michelangelo monitors both model performance and data quality in production:
Prediction Logging: A share of predictions is logged and later compared against actual outcomes, enabling live quality checks and early detection of accuracy drift.
Metrics Tracking: For example, regression models are tracked with metrics like R-squared, RMSE, and MAE. These feed into Uber’s monitoring systems, where teams can visualize trends, set alerts, and trigger corrective actions.
By combining prediction monitoring with strong pipeline checks, Michelangelo ensures Uber’s large-scale systems continue delivering reliable performance for millions of users.

 Predictions are sampled and compared to observed outcomes to generate model accuracy metrics.

Management Plane, API, and Web UI
With the scale and complexity of Uber’s infrastructure, a robust system is needed to run workflows and manage models in production. This is the role of Michelangelo’s management plane.
At its core, Michelangelo’s management plane includes:
Management Application: Powers the web UI and network API, while integrating with Uber’s monitoring and alerting systems.
User Interaction: Provides multiple ways to access the platform, whether through a visual interface, REST API, or automated alerts.
Workflow System: Orchestrates key tasks such as batch data pipelines, training jobs, prediction jobs, and deployments to both online and offline containers.

By centralizing these tasks, Michelangelo ensures scalability, efficiency, and clear oversight of the ML lifecycle.


Lessons in MLOps from Uber’s Michelangelo
From studying Michelangelo’s architecture, a few clear patterns emerge: customization to business needs, automation and seamlessness at scale, layered workflows for reliability, and institutional learning from every model.

1. Standardization as the Foundation
Before Michelangelo, Uber’s ML workflows were fragmented, causing duplicated effort and unreproducible results. The platform solved this by unifying the pipeline from data ingestion to training, deployment, and monitoring into a single six-step framework, helping Uber scale without teams rebuilding the wheel.

2. Customization for Different Needs
Uber recognized that no single architecture could fit every problem. Instead, they customized their system to address varied requirements:
Dual Pipeline Architecture: Offline (HDFS + Spark) for batch jobs like training and bulk prediction (e.g., churn prediction), and Online (Kafka + Samza + Cassandra) for real-time predictions and frequent feature updates.
Partitioned Models: Training data was split by user-defined configurations (e.g., city), with independent models powering city-specific ETAs for New York vs. Mumbai, plus fallbacks where data was sparse.
Multiple Deployment Modes: Offline batch deployment, online cluster deployment, and lightweight library deployments.
This customization was critical because Uber’s business spans rides, food delivery, and freight — each demanding different balances of speed, scale, and accuracy.





3. Automation at Scale
High-volume ML production requires reducing manual work and minimizing risk. Michelangelo automated key parts of the lifecycle:
Feature Store: Automated feature calculation and enabled shared access across teams from a single source.
Model Version Switching: UUIDs and tags allowed seamless upgrades without downtime or extra engineering workload.
Automated Hyperparameter Search: Reduced manual trial-and-error during training.
These systems ensured Uber’s ML workflows operated reliably and efficiently at massive scale while still leaving room for innovation and experimentation.

4. Monitoring and Reliability
Uber’s predictions are directly tied to customer experience, and shifts in traffic, weather, or pipeline reliability could quickly degrade accuracy. Michelangelo tackled this with continuous monitoring of both models and data pipelines:
Prediction Checks: Logged and compared against outcomes.
Metrics: RMSE, MAE, and R² tracked performance.
Pipeline Health: Consistency monitored to avoid silent failures.
Together, these safeguards kept predictions accurate, stable, and trustworthy in dynamic real-world conditions.
5. Learning from Every Model
Uber frequently trained hundreds of models, many of which never reached production. Instead of discarding these efforts, Michelangelo stored detailed metrics, configurations, and artifacts for every run in its Model Repository. By analyzing this history, Uber guided smarter decisions for future training and deployments, turning even failed experiments into valuable assets.



Final Reflection
Michelangelo’s story at Uber is more than just a machine learning platform; it’s a case study in how engineering choices shape business outcomes at scale. What began as scattered, one-off experiments evolved into a unified system serving millions of users in real time across rides, food delivery, and freight.
Its success came from a few core shifts that were deliberate business decisions, using the right technologies for maximum impact: standardizing workflows, customizing pipelines and models, automating at scale, monitoring end-to-end, and turning every experiment into a source of learning. These efforts helped Uber minimize inefficiencies that, at scale, could have caused major losses.
From this case study, the lesson for any organization is clear: MLOps isn’t only about models, pipelines, or platforms in isolation. It’s about building systems that evolve with the business, embrace experimentation, and scale without breaking. Michelangelo demonstrates how a company can transform ML from a collection of ad-hoc tools into a true competitive advantage—one that powers not just predictions, but the business itself.

CITATIONS : 
Hermann, Jeremy, and Mike Del Balso. “Meet Michelangelo: Uber’s Machine Learning Platform.” Uber Engineering Blog, 5 Sept. 2017, https://www.uber.com/en-IN/blog/michelangelo-machine-learning-platform/.

OpenAI. ChatGPT. OpenAI, 16 Sept. 2025, chat.openai.com.






